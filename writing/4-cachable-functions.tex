\chapter{Smache: Cachable Functions}
\label{chapter:smache-cachable-functions}

The goal of this thesis is to present a caching solution that is able to handle long running computations by presenting content to the user fast while addressing the challenges of cache management and efficient update propagation. Based on the overview of existing caching solutions introduces in the last chapter, we will present a solution that solves the problem for the context of the this thesis.

% TODO: Present the content of this chapter
%   - Present that the requirements

% Structure:
% - Existing approaches does not solve our problem optimally
% - Smache: Programming Model for Cachable Functions

\section{Existing Approaches are Not Sufficient}
\label{sec:existing-approaches-are-not-sufficient}

% TODO: Find shorter title for this section

% TODO: Consider moving this to the last chapter?

In the primary use case of the context in this thesis (described in section~\ref{sec:context}), the platform presents statistical information based on some advanced computation that takes a long time to compute ($>\text{ 10 sec.}$). If we want to keep the teacher's attention to the platform, we need to find a caching technique that is not depending on the time taken to compute the information. Furthermore we want to keep the information as fresh as possible.

If we consider the overview on figure~\ref{fig:existing-solutions-comparison}, we can already leave out caches that optimizes DB-queries and declared content since they do not solve the problem of caching advanced statistical computations. The solution proposed by IBM is made for declared HTML-content and not on computations based on data from a storage system. This leaves the set of approaches that are able to cache arbitrary content and HTTP-responses, but given caching arbitrary content has less assumptions, they are more attractive.

From the approaching caching arbitrary content it is desirable to have an approach with high adaptability, which was stated as one of the requirements of the system. From the approaches with high adaptability that has a response time that does not depend on the time of computation, are \emph{expiration-based} and \emph{manual trigger-based invalidation with asynchronous updates}.

From these approaches the expiration-based invalidation technique has the advantage that it doesn't require invalidation management, but it has the limitation that all cached values of the same kind are invalidated and updated at the same frequency.

If we consider use case example~\ref{example:assignment-computations}, the statistical information of current assignments are updated at the same frequency as the closed assignments, and if we want to keep the cached values for the current information up to date, we also need to update all non current information. This is not desirable with relation to efficiency since the CPU will be busy in time proportional to the time of the computing the statistical information for the assignment and the total number of assignments on the platform. In other words the number of CPU's we need to occupy for this job can be calculated by $\frac{\Delta t_c \cdot n_c \cdot u_c}{t_d}$, where $\Delta t_c$ is the time of the computation, $n_c$ is the number of computations, $\Delta u_c$ is the number of updates per 24 hours and $\Delta t_d$ is the number of seconds per 24 hours. Considering the example - if we have 500 assignments on the platform and we want to update the information every 10 minutes with a computation time of 30 seconds each, then we occupy a CPU in $\frac{60 \cdot 500 \cdot \frac{10 \cdot 60}{60 \cdot 60 \cdot 24}}{60 \cdot 60 \cdot 24}\text{ occupied CPU's} = 50\text{ occupied CPU's}$. If the amount of computational power isn't a problem, the expiration-based approach would be the best solution, but that is not the case of this thesis, where efficiency is a requirement.

The alternative with high adaptability is to use manual trigger-based invalidation with asynchronous updates after request as described in section~\ref{subsec:trigger-based-invalidation-with-asynchronous-update}. This approach has the advantage that the values are only invalidated (and thereby recomputed) when a trigger is invoked, which means the programmer has the opportunity to optimize the approach to only invalidate when it is relevant to the user e.g. when the cached value are supposed to change. Although this can be seen as an opportunity it can also be seen as a burden for the programmer to maintain these manual triggers. And by having asynchronous invalidations, the user is presented with a stale value, and only if the value is requested again after the update, the user receives the fresh value. If we consider example~\ref{example:assignment-computations} it is not unlikely that the teacher looks at an old assignment some time after it has been closed. In cases such as this, where the value is only fetched once in a while it would not be optimal to have asynchronous updates, since the value will never be close to fresh when it is actually presented.

\begin{example}
\label{example:assignment-computations}
On the Peergrade.io platform the teacher is presented with statistical information about the grades given by the students for a given assignment. These assignments are often current at specific time interval after which the assignment becomes ``closed'' and the statistical information are not updated.
\end{example}

The approaches suggested by Chris Wasik and Dan Ports focus on automatic invalidation and could probably be extended to do asynchronous updates on request to allow immediate responses by giving up the option of freshness, which would give the same guarantees as the trigger-based invalidation with asynchronous updates with additional automatic invalidation, but it would also have the same problems.

The approach best fit for the context of this thesis is to use write-through invalidation that guarantees immediate response time as well as updating the content in-place such that the content cannot become older than the time taken to compute the value.

Although this is the best fit, it still leaves a burden for the programmer to declare the triggers that invokes the write through updates. Furthermore it leaves the challenge of ensuring the integrity of the cached values and invalidation marks in concurrent environments as described on figure~\ref{fig:trigger-based-concurrency-problem}.

The goal of this thesis is to present a caching approach that solves the problem of the thesis for the use case described in section~\ref{sec:context} as well as fulfill the requirements described in section~\ref{sec:requirements}. Since the computation time in the given use case can be long, we must ensure that the response time does not depend on the computation time. Since we must respond immediately we need to trade-off strict freshness. To keep the values as up to date as possible we can use in-place updates based on trigger invalidation. As we can see on figure~\ref{fig:existing-solutions-comparison}, there exists no solutions fulfilling those criteria. In the rest of the thesis we will present the design and implementation of the solution that meets these requirements, which we call ``Smache''.

% section existing-approaches-are-not-sufficient end

\section{The Cachable Function Model}
\label{sec:the_cachable_function_model}

Smache uses the same programming model as the one described by Dan Ports~\cite{paper:liskov}. The model is organized around \emph{cachable functions}, which essentially are normal functions, where the result are \emph{memoized}\footnote{In our case: storing the result of the function and return the cached value when the function is called again with the same input}. The cachable functions are allowed to make request to the primary storage to fetch the underlying data for the function as well as calling other cached functions. This allows caching at different granularities that gives the benefit of optimizing the invalidation for different types of content.

% - Definition of cachable function (taken from Dan Ports)

\subsection{Restricted to Pure Functions}
\label{subsec:restricted_to_pure_functions}

As mentioned by Dan Ports~\cite{paper:liskov} not all functions are cachable. To be able to cache the functions they need to be \emph{pure}, which Dan Ports defines as: \myquote{[...] they [functions] must be deterministic, not have side effects, and depend only on their arguments and the storage layer state.}. Smache does not detect these properties in a function and therefore relies on the programmer to ensure only pure functions are cached.

% TODO: Write about how the solution is restricted to pure functions,
%   - copy-paste pretty much (AND REFERENCE Dan Ports)

% subsection restricted_to_pure_functions end

\subsection{Making Functions Cachable}
\label{subsec:making-functions-cachable}

The solution suggested by Dan Ports et.al.~\cite{paper:liskov} includes fully automatic invalidation. The solution suggested by this thesis achieves a semi-automatic invalidation based on dependencies declared in the procedure that makes the function cachable. This thesis will extend from Dan Ports' solution for making cached functions with arguments for declaring dependencies. The API is defined as following:

\verb$MAKE-CACHABLE(fn, input-types, relation-deps)$ $\rightarrow$ \emph{cached-fn}: Makes a function cacheable. \emph{cached-fn} is a new function that first checks the cache for the result of another call with the same arguments. If not found, it executes \emph{fn} and stores its result in the cache.

The added arguments for the procedure does not alter the behaviour of the \emph{cached-fn}, but it requires additional arguments used by the application for naming and invalidation. The \emph{input-types} are the types of the arguments given to the original \emph{fn} and the \emph{relation-deps} are declared dependencies to data sources, which cannot be derived from the input of \emph{fn}. The \emph{input-types} are used to indicate how the arguments should be interpreted with relation to naming (described in section~\ref{subsec:cache-object-localization} and used together with the \emph{relation-deps} to achieve automatic invalidation (described in chapter~\ref{chapter:invalidation}).

\subsection{Cache Object Localization}
\label{subsec:cache-object-localization}

When the application fetches and stores the content of cached objects it need to have consistent naming such that the correct cached objects are fetched and the objects does not overwrite each other when stored. To make it easier to use cachable functions this is automatically handled by Smache. The name of a given cached object computed from a cached function \emph{must be unique} such that no other cached function is able to have the same name.

The details on how this is achieved is specific to the implementation, but as an example, the Smache Python implementation uses a combination of the function name and the module in which is was defined. This becomes unique since you cannot use two modules of the same name.

% subsection cache-object-localization end

% TODO: Write something about the key-generation

% TODO: Consider having these extensions in this section (or maybe next)

% TODO: Maybe write in more details what happens in Make-Cachable (such as registering the function)

% subsection making_functions_cachable end

\subsection{Automatic Cache Invalidation}
\label{subsec:automatic_cache_invalidation}

In Dan Ports' solution the system handles all parts of cache management such that the programmer only have to define which functions needs to be cached and not define how it should be invalidated. This is a great advantage since it avoids potential bugs related to cache invalidation, but it is a trade-off for flexibility as well as adaptability since this approach moves the complexity to the database in the form of assumptions about the primary storage, cache database as well as a daemon process for managing snapshots~\cite{paper:liskov}.

Smache does not remove the burden of cache management, but it will handle naming (or localization) and invalidation by only relying on the programmer declaring dependencies to underlying data. This will improve the usability of cache management since the cache invalidation only changes when there are changes to the underlying data as described more in chapter~\ref{chapter:invalidation}.

% subsection automatic_cache_invalidation end

\subsection{Data Update Propagation}
\label{subsec:model_data_update_propagation}

Smache also offers the possibility to perform in-place updates using a data update propagation system inspired by the solution IBM used to achieve a 100\% cache hit rate on the content management system for the Olympic Games in 2000~\cite{paper:ibm, paper:ibm-extended}.

Having in-place updates allows the programmer to have immediate response times while ensuring that the values returned are updated as soon as they are invalidation such that the value is not more stale than the time taken to compute it. This will be explained more in chapter~\ref{chapter:data-update-propagation}.

% subsection data_update_propagation end

\section{Implementing Cachable Functions in Python}
\label{sec:implementing_cachable_functions_in_python}

The basic cachable functions as they are described so far can be implemented using the architecture described in section~\ref{fig:caching-basics-architecture}. Smache implements the cachable function by exposing an implementation of the \verb$MAKE-CACHABLE$. To apply the procedure the Smache library must be loaded in the application and applied. After the \verb$MAKE-CACHABLE$ function has been used, the control flow illustrated on figure~\ref{fig:cachable-function-control-flow} is applied. When a client makes a request involving a cached function, the application will call the function wrapper that tries to find the value in the cache first and if it is not present in the cache, the original function will be called after which the result is cached and returned to the caller.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/cachable-function-control-flow.pdf}
  \caption{The control flow during a call to a function cached through Smache}
  \label{fig:cachable-function-control-flow}
\end{figure*}

The procedure for making a function cached is implemented in using Python-decorators~\footnote{https://www.python.org/dev/peps/pep-0318/} that is an annotation that can be used to modify the behaviour of existing functions while being able to keep a reference to the original function.

The implementation of the basic caching on the running example is seen on code snippet~\ref{code:basic-caching-running-example}. In this code the only addition we have made is a call to a function called \verb$computed$ on a module \verb$smache$ with a single argument and a \verb$@$ as prefix that indicate it's a decorator. Additionally the there is a decorator called \verb$relation$ in which we indicate the entity type as well as a function that describes how the given relation entity relates to an entity indicated through the input. This call corresponds to the implementation of \verb$MAKE-CACHABLE$.

Smache require the programmer to annotate the types of ``data source'', which are passed into the original function. In this case we have annotated using the \verb$Course$-class that corresponds to an object for an Object-Relational Mapper (ORM) that represents the collection \emph{course}. The ``data source types'' supported are ORM-objects as well as the ``Raw''-type that represents primitive values (such as strings, numbers etc.). The number of type annotations has to be the same number of arguments given to the original function.

Smache uses the type annotations to do smart serialization and deserialization when the name of the cached value have to be inferred. The raw input are represented as they are in a serialized form, but the data source inputs are serialized using their. When the function is then deserialized the Smache library will use the id of the data sources to load a fresh version of the ORM-object from the database. Another advantage is that the Smache now know that this cached function has to be invalidated when the corresponding course entity is updated, which will be explained in chapter~\ref{chapter:invalidation}.

\begin{figure*}[ht!]
  \input{code/basic_implementation.py}
  \caption{Implementation of basic caching on the running example}
  \label{code:basic-caching-running-example}
\end{figure*}

% TODO: Write something about how the key is generated for the running example and maybe other keys

% subsection defining_the_cachable_functions end

% section implementing_cachable_functions_in_python end

%   - Discussion about guarantees (where does it lie in the comparison mode)
%   - MAYBE wait until final discussion
%     - Configurations:
%       - Always Immediate Response + Relaxed Freshness
%       - Strict Freshness

% section the_cachable_function_model end

\section{Discussion}
\label{sec:cachable-function-discussion}

% TODO: Discuss how these cachable functions solve the problem of cache maintenance

% section discussion end


% chapter smache-cachable-functions end
