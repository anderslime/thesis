\chapter{Smache: Cachable Functions}
\label{chapter:smache-cachable-functions}

The goal of this thesis is to present a caching solution that is able to handle long running computations by presenting content to the user fast while addressing the challenges of cache management and efficient update propagation. Based on the overview of existing caching solutions introduced in the last chapter, we will present a solution that solves the problem for the context and requirements of the this thesis.

In this chapter we will start by arguing why the existing caching approaches are not sufficient (section~\ref{sec:existing-approaches-are-not-sufficient}). Followed by these arguments we present the programming model that is the basis of the solution suggested for this thesis (section~\ref{sec:the_cachable_function_model}), how we implemented this model in Python (section~\ref{sec:implementing_cachable_functions_in_python}) with a final discussion on the limitations of the design and implementation of the model (section~\ref{sec:cachable-function-discussion}).

\section{Why Existing Approaches are Not Sufficient}
\label{sec:existing-approaches-are-not-sufficient}

In the primary use case of the context in this thesis (described in section~\ref{sec:context}), the platform presents statistical information based on some advanced computation that takes a long time to compute ($>\text{ 10 sec.}$). If we want to keep the teacher's attention to the platform, we need to find a caching technique that is not depending on the time taken to compute the information. Furthermore we want to keep the information as fresh as possible.

If we consider the overview on figure~\ref{fig:existing-solutions-comparison}, we can already leave out caches that optimizes DB-queries and declared content since they do not solve the problem of caching advanced statistical computations. The solution proposed by IBM is made for declared HTML-content and not on computations based on data from a storage system. This leaves the set of approaches that are able to cache arbitrary content and HTTP-responses, but given caching arbitrary content has less assumptions, they are more attractive.

From the approaching caching arbitrary content it is desirable to have an approach with high adaptability, which was stated as one of the requirements of the system. From the approaches with high adaptability that has a response time that does not depend on the time of computation, are \emph{expiration-based} and \emph{manual trigger-based invalidation with asynchronous updates}.

From these approaches the expiration-based invalidation technique has the advantage that it doesn't require invalidation management, but it has the limitation that all cached values of the same kind are invalidated and updated at the same frequency.

If we consider use case example~\ref{example:assignment-computations}, the statistical information of current assignments are updated at the same frequency as the closed assignments, and if we want to keep the cached values for the current information up to date, we also need to update all non current information. This is not desirable with relation to efficiency since the CPU will be busy in time proportional to the time of the computing the statistical information for the assignment and the total number of assignments on the platform. In other words the number of CPU's we need to occupy for this job can be calculated by $\frac{\Delta t_c \cdot n_c \cdot u_c}{t_d}$, where $\Delta t_c$ is the time of the computation, $n_c$ is the number of computations, $\Delta u_c$ is the number of updates per 24 hours and $\Delta t_d$ is the number of seconds per 24 hours. Considering the example - if we have 500 assignments on the platform and we want to update the information every 10 minutes with a computation time of 30 seconds each, then we occupy a CPU in $\frac{60 \cdot 500 \cdot \frac{10 \cdot 60}{60 \cdot 60 \cdot 24}}{60 \cdot 60 \cdot 24}\text{ occupied CPU's} = 50\text{ occupied CPU's}$. If the amount of computational power isn't a problem, the expiration-based approach would be the best solution, but that is not the case of this thesis, where efficiency is a requirement.

The alternative with high adaptability is to use manual trigger-based invalidation with asynchronous updates after request as described in section~\ref{subsec:trigger-based-invalidation-with-asynchronous-update}. This approach has the advantage that the values are only invalidated (and thereby recomputed) when a trigger is invoked, which means the programmer has the opportunity to optimize the approach to only invalidate when it is relevant to the user e.g. when the cached value are supposed to change. Although this can be seen as an opportunity it can also be seen as a burden for the programmer to maintain these manual triggers. And by having asynchronous invalidations, the user is presented with a stale value, and only if the value is requested again after the update, the user receives the fresh value. If we consider example~\ref{example:assignment-computations} it is not unlikely that the teacher looks at an old assignment some time after it has been closed. In cases such as this, where the value is only fetched once in a while it would not be optimal to have asynchronous updates, since the value will never be close to fresh when it is actually presented.
\begin{example}
\label{example:assignment-computations}
On the Peergrade.io platform the teacher is presented with statistical information about the grades given by the students for a given assignment. These assignments are often current at specific time interval after which the assignment becomes ``closed'' and the statistical information are not updated.
\end{example}
The approaches suggested by Chris Wasik and Dan Ports focus on automatic invalidation and could probably be extended to do asynchronous updates on request to allow immediate responses by giving up the option of freshness, which would give the same guarantees as the trigger-based invalidation with asynchronous updates with additional automatic invalidation, but it would also have the same problems.

The approach best fit for the context of this thesis is to use write-through invalidation that guarantees immediate response time as well as updating the content in-place such that the content cannot become older than the time taken to compute the value.

Although this is the best fit, it still leaves a burden for the programmer to declare the triggers that invokes the write through updates. Furthermore it leaves the challenge of ensuring the integrity of the cached values and invalidation marks in concurrent environments as described on figure~\ref{fig:trigger-based-concurrency-problem}.

The goal of this thesis is to present a caching approach that solves the problem of the thesis for the use case described in section~\ref{sec:context} as well as fulfill the requirements described in section~\ref{sec:requirements}. Since the computation time in the given use case can be long, we must ensure that the response time does not depend on the computation time. Since we must respond immediately we need to trade-off strict freshness. To keep the values as up to date as possible we can use on-write updates based on trigger invalidation. As we can see on figure~\ref{fig:existing-solutions-comparison}, there exists no solutions fulfilling those criteria. In the rest of the thesis we will present the design and implementation of the solution that meets these requirements, which we call ``Smache''.

% section existing-approaches-are-not-sufficient end

\section{The Cachable Function Model}
\label{sec:the_cachable_function_model}

Smache uses the same programming model as the one described by Dan Ports~\cite{paper:liskov}. The model is organized around \emph{cachable functions}, which essentially are normal functions, where the result are \emph{memoized}\footnote{In our case: storing the result of the function and return the cached value when the function is called again with the same input}. The cachable functions are allowed to make request to the primary storage to fetch the underlying data for the function as well as calling other cached functions. This allows caching at different granularities that gives the benefit of optimizing the invalidation for different types of content.

% - Definition of cachable function (taken from Dan Ports)

\subsection{Restricted to Pure Functions}
\label{subsec:restricted_to_pure_functions}

As mentioned by Dan Ports~\cite{paper:liskov} not all functions are cachable. To be able to cache the functions they need to be \emph{pure}, which Dan Ports defines as: \myquote{[...] they [functions] must be deterministic, not have side effects, and depend only on their arguments and the storage layer state.}. Smache does not detect these properties in a function and therefore relies on the programmer to ensure only pure functions are cached.

% subsection restricted_to_pure_functions end

\subsection{Making Functions Cachable}
\label{subsec:making-functions-cachable}

The solution suggested by Dan Ports et.al.~\cite{paper:liskov} includes fully automatic invalidation. The solution suggested by this thesis achieves a semi-automatic invalidation based on dependencies declared in the procedure that makes the function cachable. This thesis will extend from Dan Ports' solution for making cached functions with arguments for declaring dependencies. The API is defined as following:

\verb$MAKE-CACHABLE(fn, input-types, relation-deps)$ $\rightarrow$ \emph{cached-fn}: Makes a function cacheable. \emph{cached-fn} is a new function that first checks the cache for the result of another call with the same arguments. If not found, it executes \emph{fn} and stores its result in the cache.

The added arguments for the procedure does not alter the behaviour of the \emph{cached-fn}, but it requires additional arguments used by the application for naming and invalidation. The \emph{input-types} are the types of the arguments given to the original \emph{fn} and the \emph{relation-deps} are declared dependencies to data sources, which cannot be derived from the input of \emph{fn}. The \emph{input-types} are used to indicate how the arguments should be interpreted with relation to naming (described in section~\ref{subsec:cache-object-localization} and used together with the \emph{relation-deps} to achieve automatic invalidation (described in chapter~\ref{chapter:invalidation}).

% subsection making_functions_cachable end

\subsection{Cache Object Localization}
\label{subsec:cache-object-localization}

When the application fetches and stores cached objects, the developer must derive a name that is used as the key in the cache store and therefore also used for identifying the cached object. To localize a cached object instance correctly the name \emph{must be unique} such that no other cached function is able to have the same name. To make cache management easier cachable functions in Smache automatically generates the name based on the input given to \verb$MAKE-CACHABLE$. The details on how this is achieved is specific to the implementation. The naming is implemented using a \verb$SERIALIZABLE-FUN-NAME$ that is defined as following:

\verb$SERIALIZABLE-FUN-NAME(fn, input)$ $\rightarrow$ \emph{fun-name}: Returns the name that uniquely identifies the cached object instance representing the call to the function \emph{fn} with \emph{input}. The name is constructed by concatenating a unique serialized version of \emph{fn} and a serialization of \emph{input}. The serialization of \emph{input} is done by concatenating a serialized version of each argument.

This procedure makes it possible to represent the cached object in the cache such that it is possible to locate and store the result of a given cached function in the cache database.

To support on-write updates that allows the cache system to automatically update the cached objects as they are invalidated, \verb$SERIALIZABLE-FUN-NAME$ must also comply with the following requirement:

\begin{itemize}
  \item The serializations of \emph{fn} and \emph{input} must be done in such a way that there exists another procedure \verb$DESERIALIZE-FUN$ that deserializes \emph{fun-name} such that it is possible to locate and call \emph{fn} with \emph{input} in another process.
\end{itemize}

The \verb$DESERIALIZED-FUN$ is defined as following:

\verb$DESERIALIZED-FUN(fun-name)$ $\rightarrow$ \emph{fun}, \emph{input}: Returns a pointer to \emph{fun} as well as \emph{input}. It must be possible to call \emph{fun} with \emph{input} as arguments such that the result is the same as before \verb$SERIALIZABLE-FUN-NAME$ is applied.

% subsection cache-object-localization end

\subsection{Automatic Cache Invalidation}
\label{subsec:automatic_cache_invalidation}

In Dan Ports' solution the system handles all parts of cache management such that the programmer only have to define which functions needs to be cached and not define how it should be invalidated. This is a great advantage since it avoids potential bugs related to cache invalidation, but it is a trade-off for flexibility as well as adaptability since this approach moves the complexity to the database in the form of assumptions about the primary storage, cache database as well as a daemon process for managing snapshots~\cite{paper:liskov}.

Smache does not remove the burden of cache management, but it will handle naming (or localization) and invalidation by only relying on the programmer declaring dependencies to underlying data. This will improve the usability of cache management since the cache invalidation only changes when there are changes to the underlying data as described more in chapter~\ref{chapter:invalidation}.

% subsection automatic_cache_invalidation end

\subsection{Update On Write}
\label{subsec:write-through-updates}

Smache also offers the possibility to perform on-write updates using a data update propagation system inspired by the solution IBM used to achieve a 100\% cache hit rate on the content management system for the Olympic Games in 2000~\cite{paper:ibm, paper:ibm-extended}.

Having on-write updates allows the programmer to have immediate response times while ensuring that the values returned are updated as soon as they are invalidation such that the value is not more stale than the time taken to compute it. This will be explained more in chapter~\ref{chapter:data-update-propagation}.

% subsection write-through-updates end

\section{Implementing Cachable Functions in Python}
\label{sec:implementing_cachable_functions_in_python}

% How the cachable function works
Smache implements the cachable function by exposing an implementation of the \verb$MAKE-CACHABLE$. To apply the procedure the Smache library must be loaded in the application and applied. The \verb$MAKE-CACHABLE$ converts the original function to a cached function that intercepts the method invocation and directs it to a wrapper function in the Smache library. When a client makes a request involving a cached function, the application will call the function wrapper that tries to find the value in the cache first and if it is not present in the cache, the original function will be called after which the result is cached and returned to the caller. This control flow of a cached function is illustrated on figure~\ref{fig:cachable-function-control-flow}.

\begin{figure*}[ht!]
  \begin{center}
    \includegraphics[width=1.0\linewidth]{figures/cachable-function-control-flow.pdf}
  \end{center}
  \begin{enumerate}
    \item The caller invokes the cached function through the cached wrapper in Smache
    \item Smache sends a \verb$Lookup$ request to the Cache Database
    \item The cache database returns a cached object instance
    \item[(4)] Smache calls the original function through the application
    \item[(5)] The application executes the original function, makes the necessary calls to the primary storage and returns the result
    \item[(6)] Smache sends a \verb$Store$ request to the Cache Database with the result from the application
    \item Smache returns the value to the caller from the application
  \end{enumerate}
  \footnotesize{Steps annotated with \emph{parenthesis and dashed lines} are only executed in case of cache miss or cache failure}
  \caption{The control flow during a call to a function cached through Smache}
  \label{fig:cachable-function-control-flow}
\end{figure*}

The procedure for making a function cached is implemented in using Python-decorators~\footnote{https://www.python.org/dev/peps/pep-0318/} that is an annotation that can be used to modify the behaviour of existing functions while being able to keep a reference to the original function.

The implementation of the basic caching on the running example is seen on code snippet~\ref{code:basic-caching-running-example}. In this code the only addition we have made is a call to a function called \verb$computed$ on a module \verb$smache$ with a single argument and a \verb$@$ as prefix that indicate it's a decorator. Additionally there is a decorator called \verb$relation$ in which we indicate the entity type as well as a function that describes how the given relation entity relates to an entity indicated through the input. This call corresponds to the implementation of \verb$MAKE-CACHABLE$.

\begin{code}{Implementation of basic caching on the running example}
  \input{code/basic_implementation.py}
  \label{code:basic-caching-running-example}
\end{code}

% Declaration
Smache require the programmer to annotate the types of ``data source'', which are passed into the original function. In this case we have annotated using the \verb$Course$-class that corresponds to an object for an Object-Relational Mapper (ORM) that represents the collection \emph{course}. The ``data source types'' supported are ORM-objects as well as the ``Raw''-type that represents primitive values (such as strings, numbers etc.). The number of type annotations has to be the same number of arguments given to the original function. The type annotations are used to derive direct dependencies as explained more in section~\ref{chapter:invalidation} and to implement automatic naming.

% Naming
The Smache library implements both the \verb$SERIALIZED-FUN-NAME$ and \verb$DESERIALIZED-FUN$ to support automatic invalidation and on-write updates. The implementation works by having a \emph{seperator-token} that is used in between the concatenations. The input is deserialized and serialized using the \emph{JavaScript Object Notation (JSON)}protocol~\cite{docs:json}. Using the python implementation~\cite{docs:python-json} Smache encodes primitive Python values into JSON and decode it back from JSON to Python values. The implementation of the encoding and decoding can be seen in appendix~\ref{appendix:implementation-of-function-serialization-and-deserialization}. Besides decoding and encoding the cached object instances, the full implementation also involves storing and loading references from/to the computed functions.
To give an example, we will demonstrate the usage of the localization procedures with the default seperator token $~~~$. We can serialize the function \verb$course_score$ from the running example in code~\ref{code:running-example}. When the running example has been made cachable as in code~\ref{code:basic-caching-running-example} and we use a \verb$Course$-object from the \verb$ORM$ we can serialize and deserialize as seen in code~\ref{code:function-serialization}.

\begin{code}{Example of how function serialization and deserialization can be done in Smache}
  \input{code/function_serialization.py}
  \label{code:function-serialization}
\end{code}

% section implementing_cachable_functions_in_python end

% section the_cachable_function_model end

\section{Limitations of Cachable Functions}
\label{sec:cachable-function-discussion}

The design of the cachable functions require the programming language to support serialization and deserialization as described. In Python it is possible to localize modules and functions dynamically based on a string representation, but in other languages this could be a challenge or might even require a different solution, which could be a limitation of the design and implementation. For example statically typed languages that does not allow dynamic notations, might need a different solution, where the representations are mapped to a data structured holding references to the given functions. Furthermore the solution does not consider caching methods for objects in object oriented languages, which is considered a limitation of the solution. To cache methods of objects, the solution would have to be extended to consider the state of the object as ``input'' for the cached function.

Another limitation of the interface of cachable functions described so far is that it is only possible to use it for trigger-based invalidation and the only control for the programmer is to define the dependencies such that the cached object is invalidated appropriately. There could be cases, where the cached object were to be invalidated based on some data sources, but also be once every day as an example using expiration-based invalidation. Luckily, the interface can easily be extended to include optional parameters that would allow hybrid approaches such as these.

% section discussion end

\section{Summary}
\label{sec:cachable-functions-summary}

In this chapter we introduces the interface for cachable functions that allows a programmer to declare a function to be cachable such that the second time the function is accessed the result will be served from the cache instead of recomputing it. The naming and identification of the related cached object instance is handled transparently by the caching system. The interface also includes declaration that allows to extend cachable functions to have automatic invalidation as described in the next chapter (chapter~\ref{chapter:invalidation}) and on-write updates (chapter~\ref{chapter:data-update-propagation}).

% section summary end

% chapter smache-cachable-functions end
