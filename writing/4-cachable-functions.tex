\chapter{Smache: Cachable Functions}
\label{chapter:smache-cachable-functions}

The goal of this thesis is to present a caching solution that is able to handle long running computations by presenting content to the user fast while addressing the challenges of cache management and efficient update propagation. This chapter will present the overall solution suggested by the thesis.

In this chapter we will start by arguing why the existing caching approaches are not sufficient (section~\ref{sec:existing-approaches-are-not-sufficient}). Followed by these arguments, we present the programming model that is the basis of the solution (section~\ref{sec:the_cachable_function_model}), how we implemented this model in Python (section~\ref{sec:implementing_cachable_functions_in_python}), and a final discussion on the limitations of the design and implementation of the model (section~\ref{sec:cachable-function-discussion}).

\section{Why Existing Approaches are Not Sufficient}
\label{sec:existing-approaches-are-not-sufficient}

In the primary use case of the context in this thesis (described in section~\ref{sec:context}), the platform presents statistical information based on some advanced computation that takes a long time to compute ($>\text{ 10 sec.}$). If we want to keep the teacher's attention to the platform, we need to find a caching technique that is not depending on the time taken to compute the information. Furthermore we want to keep the information as fresh as possible.

If we consider the overview on figure~\ref{fig:existing-solutions-comparison}, we can already leave out caches that optimize DB-queries and declared content since they do not solve the problem of caching advanced statistical computations. The solution proposed by IBM is made for declared HTML-content and not on computations based on data from a storage system. This leaves the set of approaches that are able to cache arbitrary content and HTTP-responses, but given caching arbitrary content has less assumptions, they are more attractive.

From the approaches caching arbitrary content it is desirable to have an approach with high adaptability, which is one of the requirements of the system. From the approaches with high adaptability that has a response time that does not depend on the time of computation, are \emph{expiration-based} and \emph{manual trigger-based invalidation with asynchronous updates}.

From these approaches the expiration-based invalidation technique has the advantage that it doesn't require much invalidation management, but it has the limitation that all cached values of the same kind are invalidated and updated at the same frequency.

If we consider use case example~\ref{example:assignment-computations}, the statistical information of current assignments are updated at the same frequency as the closed assignments, and if we want to keep the cached values for the current information up to date, we also need to update all non current information. This is not desirable with relation to efficiency since the CPU will be busy in time proportional to the time of the computing the statistical information for the assignment and the total number of assignments on the platform. In other words the number of CPU's we need to occupy for this job can be calculated by $\frac{\Delta t_c \cdot n_c \cdot u_c}{t_d}$, where $\Delta t_c$ is the time taken to compute a single assignment, $n_c$ is the number of assignments, $u_c$ is the number of updates per 24 hours, and $t_d$ is the number of seconds per 24 hours. Considering the example - if we have 500 assignments on the platform and we want to update the information every 10 minutes with a computation time of 30 seconds each, then we occupy a CPU in $\frac{30 \cdot 500 \cdot \frac{60}{10} \codt {24}}{60 \cdot 60 \cdot 24}\text{ occupied CPU's} = 25\text{ occupied CPU's}$. If the amount of computational power isn't a problem, the expiration-based approach would be the best solution, but that is not the case of this thesis, where efficiency is a requirement.

The last alternative with high adaptability is to use manual trigger-based invalidation with asynchronous updates after request as described in section~\ref{subsec:trigger-based-invalidation-with-asynchronous-update}. This approach has the advantage that the values are only invalidated (and thereby recomputed) when a trigger is invoked, which means the programmer has the opportunity to optimize the approach to only invalidate when it is relevant to the user e.g. when the cached value are supposed to change. Although this can be seen as an opportunity it can also be seen as a burden for the programmer to maintain these manual triggers. Furthermore by having asynchronous invalidations, the user is presented with a stale value always presented with a stale value on the first request after invalidation. If we consider example~\ref{example:assignment-computations} it is not unlikely that the teacher looks at an old assignment some time after it has been closed, which might just be a single request. In this case, the cache system would serve a stale value and update the value without purpose.
\begin{example}
\label{example:assignment-computations}
On the Peergrade.io platform the teacher is presented with statistical information about the grades given by the students for a given assignment. These assignments are often current at specific time interval after which the assignment becomes ``closed'' and the statistical information are not updated.
\end{example}
The approaches suggested by Chris Wasik and Dan Ports focus on automatic invalidation and could probably be extended to do asynchronous updates on request to allow immediate responses by giving up the option of freshness, which would give the same guarantees as the trigger-based invalidation with asynchronous updates with additional automatic invalidation, but it would also have the same problems.

The deploy-time model~\cite{paper:deploy-time} and TxCache~\cite{paper:liskov} chooses consistency over ensuring immediate response times and are therefore not suitable for long running computations. TxCache shows how it achieves a higher cache hit rate by limiting the number of recomputations in each request, but it does not reach 100\% hit rate. The deploy-time model could be extended to have immediate response time by using write-through invalidation, but it uses static invalidation, which is highly coupled to the specific technology, and makes it difficult to change technologies.

The approach best fit for the context of this thesis is to use write-through invalidation that guarantees immediate response time as well as updating the content in-place such that the content cannot become older than the time taken to compute the value. Although this is the best fit, it still leaves a burden for the programmer to declare the triggers that invokes the write through updates as well as naming the cached objects. Furthermore it leaves the challenge of ensuring the integrity of the cached values and invalidation marks in concurrent environments as described on figure~\ref{fig:trigger-based-concurrency-problem}.

To goal of the thesis is to design a solution that is easy for existing applications to adapt and support caching long running computations. To support long running computations the cache must be able to serve slightly stale values to serve the result immediately, while keeping the result up to date in an efficient way. As we can see on figure~\ref{fig:existing-solutions-comparison}, there exists no solutions fulfilling those criteria, and we will therefore suggest a new solution, which we call ``Smache''.

In the rest of this thesis we will describe the design and implementation of ``Smache''.

% section existing-approaches-are-not-sufficient end

\section{The Cachable Function Model}
\label{sec:the_cachable_function_model}

Smache uses an extended version of the programming model described by Dan Ports~\cite{paper:liskov}. The model is organized around \emph{cachable functions}, which essentially are normal functions, where the result are \emph{memoized}\footnote{In our case: storing the result of the function and return the cached value when the function is called again with the same input}. The cachable functions are allowed to make request to the primary storage to fetch the underlying data for the function as well as calling other cached functions. This allows caching at different granularities that gives the benefit of optimizing the invalidation for different types of content.

% - Definition of cachable function (taken from Dan Ports)

\subsection{Restricted to Pure Functions}
\label{subsec:restricted_to_pure_functions}

As mentioned by Dan Ports~\cite{paper:liskov} not all functions are cachable. To be able to cache the functions they need to be \emph{pure}, which Dan Ports defines as: \myquote{[...] they [functions] must be deterministic, not have side effects, and depend only on their arguments and the storage layer state.}. Smache does not detect these properties in a function and therefore relies on the programmer to ensure only pure functions are cached.

% subsection restricted_to_pure_functions end

\subsection{Making Functions Cachable}
\label{subsec:making-functions-cachable}

The solution suggested by Dan Ports et.al.~\cite{paper:liskov} (TxCache) includes fully transparent automatic invalidation, where Smache has automatic invalidation, but only semi-transparent invalidation that relies on the programmer declaring dependencies to underlying data. The API for making functions cached is defined as following:

\verb$MAKE-CACHABLE(fn, input-types, relation-deps)$ $\rightarrow$ \emph{cached-fn}: Makes a function cacheable. \emph{cached-fn} is a new function that first checks the cache for the result of another call with the same arguments. If not found, it executes \emph{fn} and stores its result in the cache.

The added arguments for the procedure does not alter the behaviour of the \emph{cached-fn}, but it requires additional arguments used by the application for naming and invalidation. The \emph{input-types} are the types of the arguments (e.g. the type of entity or type of raw content) given to the original \emph{fn} and the \emph{relation-deps} are declared dependencies to data sources, which cannot be derived from the input of \emph{fn}. The \emph{input-types} are used to indicate how the arguments should be interpreted with relation to naming (described in section~\ref{subsec:cache-object-localization} and used together with the \emph{relation-deps} to achieve automatic invalidation (described in chapter~\ref{chapter:invalidation}).

% subsection making_functions_cachable end

\subsection{Cache Object Localization}
\label{subsec:cache-object-localization}

When the application fetches and stores cached objects, the developer must derive a name that is used as the key in the cache store and for identifying the cached object. To localize a cached object instance correctly the name \emph{must be unique} such that no other cached function is able to have the same name. To make it easier for the programmer to cache functions, Smache automatically generates the name based on the input given to \verb$MAKE-CACHABLE$. The localization uses the \verb$SERIALIZABLE-FUN-NAME$ procedure that is defined as following:

\verb$SERIALIZABLE-FUN-NAME(fn, input)$ $\rightarrow$ \emph{fun-name}: Returns the name that uniquely identifies the cached object instance representing the call to the function \emph{fn} with \emph{input}. The name is constructed by concatenating a unique serialized version of \emph{fn} and a serialization of the \emph{input}. The serialization of \emph{input} is done by concatenating a serialized version of each argument.

This procedure makes it possible to represent the cached object in the cache to locate and store the result of a given cached function in the cache database.

To support asynchronous write-through updates that allows the cache system to automatically update the cached objects as they are invalidated, \verb$SERIALIZABLE-FUN-NAME$ must also comply with the following requirement:

\begin{itemize}
  \item The serializations of \emph{fn} and \emph{input} must be done in such a way that there exists another procedure \verb$DESERIALIZE-FUN$ that deserializes \emph{fun-name} such that it is possible to locate and call \emph{fn} with \emph{input} in another process.
\end{itemize}

The \verb$DESERIALIZED-FUN$ is defined as following:

\verb$DESERIALIZED-FUN(fun-name)$ $\rightarrow$ \emph{fun}, \emph{input}: Returns a pointer to \emph{fun} as well as the original \emph{input}. It must be possible to call \emph{fun} with \emph{input} as arguments such that the result is the same as before \verb$SERIALIZABLE-FUN-NAME$ is applied.

% subsection cache-object-localization end

\subsection{Automatic Cache Invalidation}
\label{subsec:automatic_cache_invalidation}

Dan Ports' TxCache~\cite{paper:liskov} handles all parts of cache management transparently such that the programmer only have to define which functions needs to be cached and not define how it should be invalidated. This is a great advantage since it avoids potential bugs related to cache invalidation, but it is a trade-off for flexibility as well as adaptability since this approach moves the complexity to the database in the form of assumptions about the primary storage, cache database, and requires an external daemon process for managing snapshots~\cite{paper:liskov}.

Smache does not remove the burden of cache management, but it will handle naming (or localization) and invalidation by only relying on the programmer declaring dependencies to underlying data. This will improve the usability of cache management since the cache invalidation only changes when there are changes to the underlying data as described more in chapter~\ref{chapter:invalidation}.

% subsection automatic_cache_invalidation end

\subsection{Update On Write}
\label{subsec:write-through-updates}

Smache also offers the possibility to perform write-through invalidation using a data update propagation system inspired by the solution IBM used to achieve a 100\% cache hit rate on the content management system for the Olympic Games in 2000~\cite{paper:ibm, paper:ibm-extended}.

Having write-through invalidation allows the programmer to achieve immediate response times while ensuring that the values returned are updated as soon as they are invalidated, such that the value is not more stale than the time taken to compute it. This will be explained more in chapter~\ref{chapter:data-update-propagation}.

% subsection write-through-updates end

\section{Implementing Cachable Functions in Python}
\label{sec:implementing_cachable_functions_in_python}

% How the cachable function works
Smache implements the cachable function by exposing an implementation of the \verb$MAKE-CACHABLE$ to the programmer. To apply the procedure the Smache library must be loaded in the application and applied. The \verb$MAKE-CACHABLE$ converts the original function to a cached function that intercepts the method invocation and directs it to a wrapper function in the Smache library. When a client makes a request involving a cached function, the application will call the function wrapper that tries to find the value in the cache first and if it is not present in the cache, the original function will be called after which the result is cached and returned to the caller. This control flow of a cached function is illustrated on figure~\ref{fig:cachable-function-control-flow}.

\begin{figure*}[ht!]
  \begin{center}
    \includegraphics[width=1.0\linewidth]{figures/cachable-function-control-flow.pdf}
  \end{center}
  \begin{enumerate}
    \item The caller invokes the cached function through the cached wrapper in Smache
    \item Smache sends a \verb$Lookup$ request to the Cache Database
    \item The cache database returns a cached object instance
    \item[(4)] Smache calls the original function through the application
    \item[(5)] The application executes the original function, makes the necessary calls to the primary storage and returns the result
    \item[(6)] Smache sends a \verb$Store$ request to the Cache Database with the result from the application
    \item[7] Smache returns the value to the caller from the application
  \end{enumerate}
  \footnotesize{Steps annotated with \emph{parenthesis and dashed lines} are only executed in case of cache miss or cache failure}
  \caption{The control flow during a call to a function cached through Smache}
  \label{fig:cachable-function-control-flow}
\end{figure*}

The procedure for making a function cached is implemented using Python-decorators~\footnote{https://www.python.org/dev/peps/pep-0318/}, which is an annotation used to modify the behaviour of existing functions while being able to keep a reference to the original function.

Code snippet~\ref{code:basic-caching-running-example} shows how the decorators are applied to the running example. The only addition we have made is a call to a function called \verb$computed$ on a module \verb$smache$ with a single argument and a \verb$@$ as prefix that indicate it is a decorator. Additionally there is a decorator called \verb$relation$ in which we indicate the entity type as well as a function that describes how the given relation entity relates to an entity indicated through the input. This call corresponds to the implementation of \verb$MAKE-CACHABLE$, where the arguments given to \verb$computed$ are the \verb$input-types$ and the arguments given to \verb$relation$ are the \verb$relation-deps$.

\begin{code}{How to cache the functions from the running example using Smache.}
  \input{code/basic_implementation.py}
  \label{code:basic-caching-running-example}
\end{code}

% Declaration
Smache require the programmer to annotate the types of ``underlying data'', which are passed into the original function. In this case we have annotated using the \verb$Course$-class that corresponds to an object for an Object-Relational Mapper (ORM) that represents the collection \emph{course}. The ``underlying datatypes'' supported are ORM-objects as well as the ``Raw''-type that represents primitive values (such as strings, numbers etc.). The number of type annotations has to be the same number of arguments given to the original function. The type annotations are used to derive direct dependencies as explained more in section~\ref{chapter:invalidation} and to implement automatic naming.

% Naming
The Smache library implements both the \verb$SERIALIZED-FUN-NAME$ and \verb$DESERIALIZED-FUN$ to support automatic invalidation and on-write updates. The implementation works by having a \emph{seperator-token} that is used in between the concatenations. The input is deserialized and serialized using the \emph{JavaScript Object Notation}-protocol (JSON)~\cite{docs:json}. Using the python implementation~\cite{docs:python-json} Smache encodes primitive Python values into JSON and decode it back from JSON to Python values. The implementation of the encoding and decoding can be seen in appendix~\ref{appendix:implementation-of-function-serialization-and-deserialization}. Besides decoding and encoding the cached object instances, the full implementation also involves storing and loading references from/to the computed functions.
To give an example, we will demonstrate the usage of the localization procedures with the default seperator token \verb$~~~$. We can serialize the function \verb$course_score$ from the running example in code~\ref{code:running-example}. When the running example has been made cachable as in code~\ref{code:basic-caching-running-example} and we use a \verb$Course$-instance from the \verb$ORM$ we can serialize and deserialize as seen in code~\ref{code:function-serialization}.

\begin{code}{Example of how function serialization and deserialization can be done in Smache}
  \input{code/function_serialization.py}
  \label{code:function-serialization}
\end{code}

% section implementing_cachable_functions_in_python end

% section the_cachable_function_model end

\section{Limitations of Cachable Functions}
\label{sec:cachable-function-discussion}

The design of the cachable functions require the programming language to support serialization and deserialization as described. In Python it is possible to localize modules and functions dynamically based on a string representation, but in other languages this could be a challenge or might even require a different solution, which could be a limitation of both the design and implementation. For example statically typed languages that does not allow dynamic notations, might need a different solution, where the representations are mapped to a data structured holding references to the given functions. Furthermore the solution does not consider caching methods for objects in object oriented languages. To cache methods of objects, the solution would have to be extended to consider the state of the object as ``input'' for the cached function.

Another limitation of the interface of cachable functions described so far is that it is only possible to use it for trigger-based invalidation and the only control for the programmer is to define the dependencies such that the cached object is invalidated appropriately. There could be cases, where the cached object were to be invalidated based on some data sources, but also be updated once every day using expiration-based invalidation. Fortunately, the interface can easily be extended using decorators to include optional parameters that would allow hybrid approaches such as these.

% section discussion end

\section{Summary}
\label{sec:cachable-functions-summary}

In this chapter we argued why existing approaches does not solve the problem addressed in this thesis and presented the programming model for the suggested solution. We introduces the interface for \emph{cachable functions} that allows a programmer to declare a function to be cached. When a cached function is called the second time with the same input, the result will be served from the cache. The naming and identification of the related cached object instance is handled transparently by the caching system. The interface also includes declaration that allows cachable functions to be extended with automatic invalidation as described in the next chapter (chapter~\ref{chapter:invalidation}) and write-through invalidation (chapter~\ref{chapter:data-update-propagation}).

% section summary end

% chapter smache-cachable-functions end
