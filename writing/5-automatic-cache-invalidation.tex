\chapter{Automatic Cache Invalidation}
\label{chapter:invalidation}

In the model for cachable functions described in the previous chapter~\ref{chapter:smache-cachable-functions} we've only described how the system can make a function cachable such that Smache automatically stores and locates the cached object returned by the function. While this removes the burden of naming and localizing cached objects, it does not solve the hard problem of invalidation.

To make cache invalidation easier with Smache, we will extend this solution with a mechanism that automatically invalidates the cached objects based on dependencies declared by the programmer.

This chapter describes how the suggested system uses the declared dependencies to track updates to underlying data and invalidate cached objects correctly. The design of the solution for automatic invalidation is described in three parts. First, we introduce the data structures used to achieve fast and efficient invalidation (section~\ref{sec:simple-object-dependence-graph}). Secondly, we describe how the cached objects are registered into the ODG (section~\ref{sec:dependency-registration}). Finally, we describe how Smache invalidates affected cached objects when changes are made to the underlying data (section~\ref{sec:invalidation-propagation}). After the design of the solution, we explain how we implemented automatic invalidation in Python and ends the chapter with a brief summary.

% section dependency-registration end

\section{Simple Object Dependence Graph}
\label{sec:simple-object-dependence-graph}

The Object Dependence Graph (ODG) was first described in a series of papers by IBM~\cite{paper:ibm, paper:ibm-extended, paper:ibm-publishing-system} and designed for the content management system used for the Olympic Games in 2000. In this solution, the final content served to the user is build from HTML-fragments and HTML-pages editable by the users as well as underlying data periodically changing. To be able to serve the final documents fast, the system pre-generates the HTML pages such that the system doesn't have to generate the pages on each request. To do this efficiently, the system maintains dependencies between the different kinds of objects and updates depending objects when underlying data changes. The cache object pre-generation can also be described in terms of caching, where the system uses an write-through invalidation approach.

Jim Challenger et.al.~\cite{paper:ibm-extended} presents a simple and a generalized version of the ODG. The generalized version is described for a content management system and includes a number of enhancements to support the requirements of the CMS. These enhancements are not necessary to solve the problem of this thesis, and will therefore use the simple ODG that is described as following:

ODG can be represented using a directed graph, where a vertex either represents underlying data or an object that is a transformation of underlying data. An edge from a vertex representing underlying data $u$ to a vertex representing an object $o$ denoted ($u$, $o$) indicates that a change to $u$ also affects $o$.~\cite{paper:ibm-extended} gives the following constraints for the simple ODG:

\begin{itemize}
  \item Each vertex representing underlying data does not have an incoming edge
  \item Each vertex representing an object does not have an outgoing edge
  \item All Vertices in the graph correspond to underlying data (nodes with incoming edges) or objects (nodes with an outgoing edge)
  \item None of the edges have weights associated with them
\end{itemize}

Figure~\ref{fig:simple-odg} illustrates an instance of a simple ODG with four vertices of underlying data ($u_1$, $u_2$, $u_3$ and $u_4$), two vertices representing objects ($o_1$ and $o_2$), and the dependencies between them. We can see that when underlying data $u_2$ changes, the system must update both the cached object of $o_1$ and $o_2$ and when $u_4$ changes, it only needs to update $o_2$.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.6\linewidth]{figures/simple-odg.pdf}
  \caption{An example instance of a simple ODG}
  \label{fig:simple-odg}
\end{figure*}

% section simple-object-dependence-graph end

\section{Dependency Data Structures}
\label{sec:dependency-data-structure-for-cachable-functions}

If we consider the techniques proposed for dependency management in existing solution, which we compared on figure~\ref{fig:dependency-management-comparison}, the database transaction technique allows for the most transparent dependency declaration and require no effort from the programmer, but it makes assumptions about how the database technology works, which makes it more difficult to make adaptable. Declaring dependencies in the content is not appropriate for caching functions and the technique with code generation requires lots of effort to implement the static analysis, which will only work for a specific programming language and database query language. Smache therefore uses a modification the last technique, where dependencies are declared in code as suggested by Gupta et.al.~\cite{paper:cache-genie} using variants of the Simple ODG.

There exists two representations of dependencies for cachable functions. The first one is the static representation, which can be derived directly from the declarations in the source code. This representation only describes dependencies between the declarations and not cached objects. We will define this representation as the \emph{Declaration Dependence Graph (DDG)}. The other representation is dynamic and describes the dependencies between the entities of the underlying data and the cached objects. We define this as the \emph{Instance Dependence Graph (IDG)}.

\subsection{Declaration Dependence Graph}
\label{subsec:declaration-dependence-graph}

When the cachable functions are defined as in section~\ref{subsec:making-functions-cachable}, the dependencies are declared using the entity types. The DDG is an extension of the Simple Object Dependence Graph, where the cached functions corresponds to the object vertices and the entity types corresponds to the underlying data. Where the simple ODG only includes direct dependencies, the DDG has two kind of dependencies. An edge from $u$ to $o$ denoted $(u, o)_d$ represents a direct dependency, which means that the dependencies identifies the given $o$. The direct dependencies includes an index indicating a position such that there is an order of direct dependencies for a given cached object. An edge from $u$ to $o$ denoted $(u, o)_l$ represents a lazy dependency, which are used for the dependencies that changes over the lifetime of the cached object. The lazy dependencies are declared using procedures relating to direct dependencies and are therefore evaluated every time there is an update to underlying data. Alternatively we could manage these dynamic dependencies in the data structure, which could make the evaluation more efficient, but to reduce the complexity by limiting the amount of state, we evaluate the declarations every time the related underlying data is updated.

Figure~\ref{fig:declaration-dependence-graph} shows the DDG for the running example from code snippet~\ref{code:running-example}. We see that the cached object for the \verb$participant_score$-function depends directly on the participant and it depends lazily on the grades. This is because there exists a participant for each score, but the grades for a given participant changes in the lifetime of the cached score and is therefore lazily evaluated.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/declaration-dependence-graph.pdf}
  \caption{The Declaration Dependence Graph of the running example}
  \label{fig:declaration-dependence-graph}
\end{figure*}

The DDG graph is represented using two data structures. The lazy dependencies are represented using an outgoing adjacency list from the underlying data nodes. The direct dependencies are represented using an incoming adjacency list from the object nodes ordered by the position index of the dependency. To access a given adjacency list fast we use hash tables indexed by entity type for the outgoing adjacency list and by the object ID for the incoming adjacency lists as depicted on figure~\ref{fig:ddg-data-structure}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/ddg-data-structure.pdf}
  \caption{An illustration of the data structure representing the DDG on figure~\ref{fig:declaration-dependence-graph}}
  \label{fig:ddg-data-structure}
\end{figure*}

The advantage of the DDG is that it does not change in the lifetime of the application, which means it can be preprocessed when the application starts and it does not have to support update queries.

The DDG is build by iterating through all the cached functions with their respective dependencies and add the dependencies to the data structure. The lazy dependencies are added with the entity type, the cached function and the procedure to evaluate it. The direct dependencies are added using with the cached function and the entity type. When a new dependency is added to the data structure, we lookup the indexed value in the hash table and if there exists an adjacency list, we add the dependency to the list. If there are no list, we create a new adjacency list with the given dependency as the only value.

The queries needed to find dependencies for automatic invalidation are the following:

\begin{itemize}
  \item \verb$lookup_lazy_dependencies(entity_type)$: Finds all lazy dependencies from a given entity type
  \item \verb$lookup_direct_dependencies(fun_id)$: Finds all direct dependencies from a given function id
\end{itemize}

Using these data structures we can access a pointer with access to all lazy or direct dependencies using $O(1)$ expected time if we use a hash table using perfect hashing.~\cite{paper:perfect-hashing}. In worst case the space used is the maximum number of edges $O(|u| \cdot |o|)$, where $|u|$ denotes the number of nodes representing underlying data and $|o|$ represents the number of nodes representing cached objects. In our case the number different underlying data is the number of different entity types and the number of different cached objects are the number of different cached function definitions.

% subsection declaration-dependence-graph end

\subsection{Instance Dependence Graph}
\label{subsec:instance-dependence-graph}

The Instance Dependence Graph (IDG) is also an extension of the simple Object Dependence Graph (ODG), where the data entities corresponds to underlying data and the cached object instance generated from the cachable functions. An edge from underlying data $u$ to an object $o$ denoted $(u, o)_i$ indicates that if $u$ is changed then $o$ must be updated.

The instance dependence graph for the running example is illustrated on figure~\ref{fig:instance-dependence-graph}. In the given example, this graph is quite primitive. It's worth noting that the edges in the IDG are instance specific representations of the direct dependencies of the DDG seen on figure~\ref{fig:declaration-dependence-graph}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/instance-dependence-graph.pdf}
  \caption{An example of an Instance Dependence Graph based on the running example, where we have a single course entity that have two participant entities.}
  \label{fig:instance-dependence-graph}
\end{figure*}

The representation of the IDG is similar to the representation of lazy dependencies for the DDG, where the dependencies are represented using an outgoing adjacency list indexed by a hash table as illustrated on figure~\ref{fig:idg-data-structure}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/idg-data-structure.pdf}
  \caption{An illustration of the data structure representing the IDG on figure~\ref{fig:instance-dependence-graph}}
  \label{fig:idg-data-structure}
\end{figure*}

New dependencies can be added to the IDG using, the following update query:

\begin{itemize}
  \item \verb$add_dependency(entity_id, o_id)$: Adds a dependency between the entity id \verb$entity_id$ used as index for the hash table and the cached object id \verb$o_id$ used as element in the adjacency list.
\end{itemize}

When \verb$add_dependency$ is used we check if there already exists an adjacency list in the hash table using \verb$entity_id$. If there exists one, we add \verb$o_id$ to the given adjacency list, or else we add a new adjacency list only including \verb$o_id$ indexed by \verb$entity_id$.

The queries needed for automatic invalidation are:

\begin{itemize}
  \item \verb$lookup_cached_objects(entity_id)$: Finds all cached objects depending on the \verb$entity_id$.
\end{itemize}

If we use perfect hashing as with the DDG we will also be able to lookup instance dependencies using $O(1)$ expected time. If we denote the total number of direct dependency declarations for all the cached functions as $|d|$ then the IDG uses $O(|d|)$ space in worst case.

% subsection instance-dependence-graph end

% section dependency-management-for-cachable-functions end

\section{Dependency Registration}
\label{sec:dependency-registration}

For the application to know what and when to invalidate, the application must register the dependency declarations and names of cached objects. The dependency declarations that defines the lazy and direct dependencies are part of the source code and therefore available when the application is started as illustrated on figure~\ref{fig:declaration-dependency-registration}. This registration flow collects all the cached functions registered through the cachable function procedure and adds them as dependencies in the DDG.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/declaration-dependency-registration.pdf}
  \caption{The flow in which lazy and direct dependencies are registered from the declarations}
  \label{fig:declaration-dependency-registration}
\end{figure*}

The registration of cached object instances happens when a cached object is accessed for the first time as illustrated on figure~\ref{fig:instance-dependency-registration}. In this flow we start by looking up the given cached object in the IDG, but since it's the first time the object is accessed it does not exist.

To generate the name of the cached objects we use the ordered direct dependencies from the DDG combined with the input from the request. We derive dependencies to underlying data from the input that represents entities and add dependencies between the given entities and the cached object through the IDG.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/instance-dependency-registration.pdf}
  \caption{The flow in which cached object instances are accessed when they are accessed the first time}
  \label{fig:instance-dependency-registration}
\end{figure*}

\section{Invalidation Propagation}
\label{sec:invalidation-propagation}

The purpose of invalidation is to be able to evaluate the freshness of a given cached object and know which cached objects are fresh and which are stale and need to be updated. A cached object is considered stale when its underlying data has been updated, which happens during the request from a client that involves updating, inserting or deleting data in the primary storage. To be able to react to these events, Smache subscribes to callbacks from the database wrapper. When the database transaction succeeds the database wrapper will notify Smache with the id and type~\footnote{Type is another word for the relation in a relational database and a collection in a document-oriented database} of the manipulated entity. This notification will trigger cache invalidation through Smache. Smache receives the type and id of the updated entity, and runs the algorithm described on figure~\ref{fig:invalidation-propagation}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/invalidation-propagation.pdf}
  \begin{enumerate}
    \item The invalidation trigger notifies Smache about an update to underlying data with type and id of entity.
    \item Smache queries the IDG using \verb$lookup_cached_object$ with the updated entity id to find instance dependencies>
    \item Smache queries the DDG using \verb$lookup_lazy_dependencies$ with the update entity, and evaluates the procedures for the returned lazy dependencies.
    \item Smache queries the IDG using \verb$lookup_cached_object$ with the set of entities returned from the lazy dependency procedures.
    \item The cached objects from instance and lazy dependencies are invalidated using \emph{timestamp invalidation}
  \end{enumerate}
  \caption{The invalidation propagation algorithm for Smache.}
  \label{fig:invalidation-propagation}
\end{figure*}

Since the process of finding keys for depending objects involves multiple network calls to the primary storage, the invalidation propagation must be asynchronous to avoid a performance degrade for requests that involves updating underlying data.

% Analyze the queries performed in the algorithm as prequel to the next chapter

\subsection{Timestamp Invalidation}
\label{subsec:timestamp-invalidation}

To invalidate correctly, the technique for invalidating cached objects require the following liveness and safety property:

\begin{itemize}
  \item \textbf{Safety}: \emph{The value representing a fresh cached object must be based on the newest version of the underlying data}
  \item \textbf{Liveness}: \emph{A cached object with a stale value must eventually be invalidated}
\end{itemize}

In trigger-based invalidation where the name of a cached object is considered static we need additional information to evaluate whether a given cached object is fresh. In a naive invalidation technique, we can use a boolean value that indicates whether or not the cached value is fresh, which would be correct in an where invalidations are processed sequentially. When an invalidation is triggered the value is set to \verb$false$ and when the cached object has been updated it would be set to \verb$true$. The problem here is that in an environment with multiple application servers, the technique would be prone to race conditions as illustrated on figure~\ref{fig:trigger-based-concurrency-problem}. Since the given cached object would be marked as fresh even though the value of the object in reality is stale, the cached object will not be invalidated until a new trigger is invoked and thereby contradict the liveness property.

One solution to solve this problem is to lock the invalidation mechanism such that the invalidation indication cannot be changed during an update. By using a lock the technique gets atomic updates across invalidation and value updates and thereby achieves liveness, but it also means that when another process wants to invalidate, it has to wait until the lock has been released. If the waiting process is a web server serving a user, the user would have to wait for the computation to finish.

To accommodate the liveness property and avoid user's having to wait for invalidation, we use a solution suggested in~\cite{paper:ibm-extended} that uses a form of logical timestamps to represent the version of a cached object. In this technique the cache system keeps track of the number of times a given cached object has been invalidated, which we denote $num\_of\_updates$. The number starts with $0$ and is incremented every time a cached object's underlying data changes.
We also keep a timestamp representing the number of invalidations for the given cached object before the computation of the last object, which we denote we $last\_update\_timestamp$. When the cached object is recomputed, the $num\_of\_updates$ is fetched and set as the timestamp of the current computation denoted $current\_computation\_timestamp$. When the computation returns the new value for the cached object, we write update the cached object unless it has been updated during the computation. We can do this using the following algorithm:

\begin{enumerate}
  \item Fetch the value for $num\_of\_updates$ again denoted $latest\_num\_of\_updates$.
  \item If $latest\_num\_of\_updates < current\_computation\_timestamp$ then we update $last\_update\_timestamp$ to be equal to $current\_computation\_timestamp$ and update the cached value. Else we do nothing.
\end{enumerate}

We are able to evaluate the freshness of a cached object as following:

\begin{itemize}
  \item \textbf{A cached object is fresh} when\\$num\_of\_updates = last\_update\_timestamp$
\end{itemize}

To avoid race conditions the execution of this algorithm need to be executed in a transaction such that if $num\_of\_updates$ is incremented between step 1 and 2 then the algorithm be aborted. We assume that the execution of a computation at a given timestamp always yields the same result, which means we can retry the update algorithm with the same result. In other words the algorithm is considered idempotent, and we can therefore retry the update algorithm until it has finished. Alternatively we could lock all the values while executing the algorithm, but this means we will block the incremental of $num\_of\_updates$ that is responsible for invalidating, which we would like to be fast.
Furthermore the algorithm require the incremental of $num\_of\_updates$ to be atomic.

\subsubsection{Proof}
\label{subsubsec:proof}

Since the technique has not been proved for correctness in~\cite{paper:ibm-extended} and because the correctness of Smache relies on this technique, we will give a proof of correctness for timestamp invalidation by proving the liveness and safety requirements described above. In these proves we assume that the invalidation and update algorithms are executed atomically.

\textbf{Safety}

We will prove \emph{safety} using contradiction by \emph{assuming we have a cached object considered fresh that holds a stale value stored in the cache}.

\begin{enumerate}
  \item We assume that $num\_of\_updates = t_i$ and since we know that a cached object is considered stale when $num\_of\_updates = last\_update\_timestamp$, we also have that $last\_update\_timestamp = t_i$
  \item For this to happen, a computation $f_i$ starting when $num\_of\_updates$ was equal to $t_i$ ended up updating the cached value to be $v_i$ and $last\_update\_timestamp$ to be $t_i$.
  \item Since $v_i$ by definition is a fresh value, because the value of $num\_of\_updates$ has not changed during the execution of $f_$, the value of the cached object can only become stale in on of the following cases:
    \begin{enumerate}
      \item[a)] Another computation $f_j$ updated the cached object with a stale value after the computation from step 2 updated the cached object
      \item[b)] The underlying data was updated after the computation from step 2 was started
    \end{enumerate}
  \item If the computation $f_j$ resulted in a stale value it must have been started at a point where $num\_of\_updates$ was equal to $t_j$, where $t_j < t_i$. This would contradict the algorithm since $f_j$ would not update the cached object with its stale value since $num\_of\_updates$ must have been equal to $t_i$ after the update from $f_i$ and since $t_i > t_j$.
  \item If the underlying data was updated after the computation was started when $num\_of\_updates$ was equal to $t_i$ then $num\_of\_updates$ must have been updated to be $t_k$, where $t_k > t_i$ after which we have a contradiction of the original assumption since $last\_update\_timestamp = t_i$ and the cached object is therefore not fresh since $num\_of\_updates \neq last\_update\_timestamp$ since $t_k \neq t_i$.
\end{enumerate}

\textbf{Liveness}

The value of a cached object becomes stale, when the underlying data has been updated. We therefore need to prove that an update underlying data affecting a cached object considered fresh, eventually results in the cached object becoming stale. We will also prove this by contradiction using the \emph{assumption that an update to underlying data affecting a fresh cached object, resulted in the object having a stale value and still be considered fresh}.

\begin{enumerate}
  \item We know that the algorithm eventually triggers an invalidation that will increment $num\_of\_updates$ and we know that $last\_update\_timestamp$ cannot be set to a value larger than $num\_of\_updates$.
  \item That is we have a moment immediately after $num\_of\_updates$ was incremented, where $num\_of\_updates > last\_update\_timestamp$.
  \item The only case in which the cached object can be considered fresh and still have a stale value after this moment of staleness, would be if the execution of a cached object update resulted in an update to set $last\_update\_timestamp$ equal to $num\_of\_updates$ without updating the value of the cached object. This contradicts the algorithm of timestamp invalidation, because we only update $last\_update\_timestamp$ while updating the cached value.
\end{enumerate}

% subsubsection proof end

% subsection timestamp-invalidation end

\subsection{Data Maintained by The Cache}
\label{subsec:data-maintained-by-the-cache}

To support timestamp invalidation we will extend the representation of a cached object with the following values:

\begin{itemize}
  \item $value$: The result of the cached function related to the cached object
  \item $num\_of\_updates$: The number of updates the given cached object is aware about
  \item $last\_update\_timestamp$: The timestamp corresponding to the computation of the current $value$
\end{itemize}

By representing the timestamps in the same object as the value we can more easily implement concurrency control mechanisms such as a lock or transactions as described above. We could represent the timestamps in another database, but this would mean we had to support distributed transactions across the cache database and the database with timestamps. We could also represent the timestamps in seperate objects than the value, but this would make it harder to shard the cached objects across multiple cache servers since we need to ensure that the cached objects are on the same server as the value to avoid distributed locking.

% subsection data-maintained-by-the-cache end

\subsection{Database Wrapper Triggers}
\label{subsec:database-wrapper-triggers}

% Trigger Invalidation Through Database Wrapper
% - Discuss the different alternatives and say why we want that

We've already discussed existing approaches for triggering invalidation in section~\ref{subsubsec:triggering-cache-invalidation} with a comparison on figure~\ref{fig:invalidation-trigger-comparison}. The triggers sent directly from the database or through a database sniffer have an advantage of capturing all changes made to the database - also those not sent through the application. The disadvantage of those techniques are that they are coupled to the database technology used. Instead of using a technique that depends on the exact database technology, Smache uses the database wrapper to trigger invalidations. The database wrapper makes it easier to change the implementation and thereby the database technology, which means the caching system becomes more flexible and easier to use in applications using different technologies as well as to change the technologies in the future. Furthermore since the triggers are intercepted through function callbacks in the web server process, the system doesn't need an external process to convert triggers from the database or sniffer into invalidations.

% subsection database-wrapper-triggers end

\section{Implementing Automatic Invalidation}
\label{sec:implementing-automatic-invalidation}

The Smache library implements automatic invalidation as described in the design above except for a few details such as the exact data structures of the dependency graphs, which have been replaced by similar representations to simplify the implementation. In this section we explain how the implementation of cachable functions (section~\ref{sec:implementing_cachable_functions_in_python}) is extended to support automatic invalidation. We will start by explaining how the IDG and DDG are implemented (section ~\ref{subsec:dependency-graphs}) followed by a description on how we implement transactions to update cached objects with timestamp invalidation (section~\ref{subsec:update-transactions}). Finally, we explain how we implement asynchronous invalidation to avoid affecting the performance of requests updating underlying data (section~\ref{subsec:asynchronous-invalidation}).

\subsection{Dependency Graphs}
\label{subsec:dependency-graphs}

The dependency graphs are used by the update procedure to derive affected cached objects to be invalidated.

The DDG is static and will not change after the application starts and we can therefore easily duplicate the data across all the web server without implementing any replication. The DDG is therefore represented using objects in the application that can be queried by calling the methods of the DDG repository.

On the other hand, the IDG is dynamic in the lifetime of the application and will therefore be represented in a separate process. Optimally the IDG should be represented using hash tables, but to simplify the architecture, the data structure is implemented using the cache database that is assumed to be a common cache server supporting \verb$LOOKUP$ and \verb$STORE$ as described in section~\ref{subsec:architecture}. More specifically the current implementation uses \emph{Redis} for storing dependencies. In this implementation the keys corresponds to keys in the hash table and the linked lists are represented in the content by a serialized list.

Alternatively we could represent the dependency graphs using a cache manager process, but this would just introduce an additional potential bottleneck as well as a single point of failure. In our solution we limit the number of possible process failures by integrating the dependency graph into existing components of the architecture.

% subsection dependency-graphs end

\subsection{Update Transactions}
\label{subsec:update-transactions}

To implement timestamp invalidation we need to implement concurrency control to ensure that the timestamp invalidation is implemented correctly. In the version of Smache implemented during this thesis, we use \emph{Redis}~\footnote{http://redis.io/} as the cache database. Redis have support for simple transactions using a combination of the \verb$WATCH$-command and the \verb$MULTI$-command. The \verb$WATCH$-command is executed with a key, which tells Redis that a given transaction should fail if the specificed key is modified during the transaction. The \verb$MULTI$-command allows to specify multiple commands in a single atomic action.
Code snippet~\ref{code:timestamp_invalidation} is a simplified version of the code that implements the update procedure with timestamp invalidation. It uses the Python Redis~\cite{docs:python-redis} library to communicate with the Redis database. When it invokes the \verb$WATCH$-command the client will request Redis to start a transaction. Afterwards we build a \verb$MULTI$-command that will update the cached object if it is newer and invokes the \verb$EXECUTE$-command. The Redis-client will send the command and if the transaction fails it will raise a \verb$WatchError$-exception indicating the key has been modified, after which we retry the transaction with the same values.

\begin{code}{The code for implementing timestamp invalidation in Python. RedisConnection represents an object communicating with the (Redis) cache database. CacheStore represents the object that communicates with the cache database.}
  \input{code/timestamp_invalidation.py}
  \label{code:timestamp_invalidation}
\end{code}

% subsection update-transactions end

\subsection{Asynchronous Invalidation}
\label{subsec:asynchronous-invalidation}

Recall figure~\ref{fig:automatic-invalidation-flow} illustrating the flow where changes to the data in the primary storage system triggers a notification to the caching system, which invalidates affected cache objects. If the notification in step 3 was invoked synchronously then the performance of step 4 and 5 would affect the response time of the request and thereby affect the user experience of the use case involving the given request. In our case we cannot ensure to retrieve dependencies fast since we have lazy dependencies that must be evaluated based on requests to the primary storage. We will therefore implement the trigger-step asynchronously such that requests involving updates to underlying data are not affected by Smache.

If the notification is delivered more than once we will just achieve a worse cache hit rate and affect performance, but it will not affect the integrity of the cache object. We must therefore monitor and ensure exactly-once-delivery of the notification.
With most technologies the simplest solution for implementing asynchronous behaviour is to start a new thread that executes the invalidation. This way the asynchronous behaviour is controlled by the web server and we don't have to introduce new components to the architecture. Although this would be the preferred solution the Python language does not implement real concurrent behaviour, because it implements a Global Interpreter Lock that prevents multiple threads to execute the same bytecode at once.
To implement concurrent invalidations in Python we will therefore use the concept of \emph{Background Jobs}, where the notifications are represented by a \emph{job} that is pushed into a queue. To execute the jobs we have multiple \emph{workers}, which are basically processes that also contains a version of the source code for the application. When there are jobs in the queue, a worker will pull the given job from the queue and execute it. Figure~\ref{fig:background-workers} illustrates how background workers are used to invalidate cache object asynchronously. To implement this behaviour we need to add application workers to the architecture as illustrated on figure~\ref{fig:architecture-with-workers}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/smache-architecture.pdf}
  \caption{The architecture required by a web application that uses Smache with automatic invalidation.}
  \label{fig:architecture-with-workers}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/background-workers.pdf}
  \caption{How background workers are used do perform invalidation asynchronously.}
  \label{fig:background-workers}
\end{figure*}

% subsection asynchronous-invalidation end

% section implementing-automatic-invalidation end

\section{Summary}
\label{sec:invalidation-summary}

In this chapter we suggest an extension to cachable functions that provides automatic invalidation. The caching system uses the dependency information given as arguments, when a function is made cachable, to derive the dependency strategy for how and when to invalidate the related cached object. To provide efficient and fast invalidation, the caching system uses extensions of the Simple Object Dependence Graph data structure called Declaration Dependence Graph (DDG) and Instance Dependence Graph (IDG), to access the dependency information fast.
The caching system registers static dependency information stored in the DDG when the application starts and dynamic dependency information in the IDG when a cached function is called.
When underlying data are changed, the caching system will query the dependency data structures to find the cached object instances affected by the change, which are then invalidated using timestamp invalidation. To verify correctness of the solution, we proved safety and liveness of the timestamp invalidation technique.
The automatic invalidation was implemented in Python and used Redis to represent dependency graphs and to facilitate update transactions required for timestamp invalidation. To avoid affecting the performance of existing update operations in the application, the invalidation are performed asynchronously using the \emph{Background Jobs}.

% section summary end

% chapter automatic_cache_invalidation end

