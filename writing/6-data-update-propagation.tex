\chapter{Data Update Propagation}
\label{chapter:data-update-propagation}

Until now we've explained how to build a caching system that is able to cache the result of functions, where the result is invalidated automatically when its underlying data changes. This solution makes it easier to manage cache invalidation and locate cached values, but after a result has been invalidated the user has to wait for the value to be recomputed, which can be critical for the user experience in cases where the computation time is too long.
In this chapter we will extend the current solution with in-place updates using a data update propagation algorithm that schedules updates for invalidated cache objects addressing the second challenge of the problem description~\ref{sec:problem}.

% Content in this chapter:
% - Analysis of the concurrency problem
% - Efficient schedueling of updates
%   - Look for similar solutions in research!
%     => "DBProxy: A dynamic data cache for Web applications"
%     => "Scalable application-aware data freshening"
%     => ?
%   - Avoid unnecessary updates
%     => Prune duplicate jobs? (spoiler: NOT)
%     => Do not compute fresh values (YES!)
%   - IBM:
%     => Schedule in topological order + parallelize sub-graphs
%   - Others:
%     => "Update Propagation Strategies for Improving the Quality of Data on the Web "
%   - Our solution:
%     => Using timestamp invalidation for concurrency control also for in-place updates
%     => Always compute newest value (do not use stale value of sub-computes)
%     => We get: retries, parallel computations, simple scheduling
%   - Maybe measure QoD?

\subsection{Consistent Concurrent Write-Through}
\label{subsec:consistent_concurrent_write_through}

% TODO: Write this as a problem that is ignored in many existing caching solutions or maybe just traded off for simplicity. Describe how key-based invalidation ensures this and how all other could have this problem solved.

At first we would like the caching to be correct. In the case of caching, we will define correctness in terms of liveness and safety: the caching system will update the cache when necessary and it will eventually return the most fresh value computed. That is if we have a computation $f$ that computes the value $v_1$ at time $t_1$ and $v_2$ at time $t_2$ then the cache store will eventually contain $v_2$ given that $t_2 > t_1$. Although this could be seen as a prerequisite for the caching system, most implementations ignore this fact to achieve a simpler cache system. Because when we want to keep the integrity between updates we need some kind of ordering for the updates that adds complexity to the system. The problem is also illustrated on figure~\ref{fig:incorrect-updates-analysis}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/incorrect-update-analysis.pdf}
  \caption{Showing how two concurrent caching updates from two different application servers results in an inconsistent state. We see that even though the request from \emph{Web 2} are based on data older than \emph{Web 1} it gets to write }
  \label{fig:incorrect-updates-analysis}
\end{figure*}

% TODO: Analysis and argue why some of the solutions leads to inconsistent cached values and breaks the safety requirement.

% subsection consistent_concurrent_write_through end

% section updating_the_cache end
