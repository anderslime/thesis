\chapter{Data Update Propagation}
\label{chapter:data-update-propagation}

Until now we've explained how to build a caching system that is able to cache the result of functions, where the result is invalidated automatically when its underlying data changes. This solution makes it easier to manage cache invalidation and locate cached values, but after a result has been invalidated the user has to wait for the value to be recomputed, which can be critical for the user experience in cases where the computation time is too long.

In this chapter we will extend the current solution with write-through invalidation using a data update propagation (DUP) algorithm that schedules updates for invalidated cache objects addressing the second challenge of the problem description~\ref{sec:problem}. We will start by covering existing approaches for data update propagation (section~\ref{sec:existing-data-update-propagation-approaches}), followed by an analysis of the problems involved with concurrent write-through updates (section~\ref{sec:race-condition-on-write-through-invalidation}). Based on the knowledge from these sections we will describe the design (section~\ref{sec:the-data-update-propagation-algorithm}) and implementation (section~\ref{sec:implementing-the-data-updata-propagation-algorithm}).

\section{Existing Data Update Propagation Approaches}
\label{sec:existing-data-update-propagation-approaches}

In the approach suggested by Jim Challenger et.al.~\cite{paper:ibm, paper:ibm-extended, paper:ibm-publishing-system}, the DUP algorithm is based on invalidation using an Object Dependence Graph as described in section~\ref{sec:simple-object-dependence-graph}. When underlying data changes all depending cached objects are scheduled for update. Because some of the cached objects depend on each other, the scheduling have to enforce some ordering to compute the values correctly. If a cached object $o_2$ that depends on another cached object $o_1$ were updated first, then it would be based on an old version of $o_1$, which means it results in a stale value and thereby violates safety. To solve this problem, the system updates all cached objects in a topological order, which ensures that $o_2$ is ordered after $o_1$ since it depends on $o_1$. One limitations of this technique is that it only works if the object dependence graph has no cycles i.e. it is an \emph{Directed Asyclic Graph}. Another limitation is that when there is an order of the jobs then they must be synchronized when executed in parallel.

Labrinidis et.al.~\cite{paper:update-propagation-strategies} suggests an algorithm that schedules cache updates prioritized by a Quality of Data measurement, which they define as the probability that a request results in a fresh value from the cache. The algorithm is then designed to maximize the overall Quality of Data by continuously evaluating the measurements and scheduling the updates in a prioritized order.

% section existing-data-update-propagation-approaches end

\section{Concurrency Analysis of Write-Through Invalidations}
\label{sec:race-condition-on-write-through-invalidation}
Since the cached objects are accessed by multiple processes they become a shared resource, and we therefore have to consider concurrency challenges such as potential race conditions. Figure~\ref{fig:incorrect-updates-analysis} illustrates a potential race condition that can occur, when multiple processes processes tries to update the same cached objects at the same time. In this case a race condition affects the correctness of the system such that a given cached object is marked as fresh even though it's value is stale, and thereby violates the safety requirement.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/incorrect-update-analysis.pdf}
  \caption{Showing how two concurrent caching updates from two different application servers results in an inconsistent state. We see that even though the request from \emph{Update Process 2} are based on data older than \emph{Update Process 1} it gets to write.}
  \label{fig:incorrect-updates-analysis}
\end{figure*}

% section race-condition-on-write-through-invalidation end

\section{Design of the DUP Algorithm}
\label{sec:the-data-update-propagation-algorithm}
The existing solution described in section~\ref{sec:existing-data-update-propagation-approaches} assumes that a given cached computation is only accessed by one process at the same time. These techniques have the advantage that they avoid race conditions, but in order to optimize with concurrency, they have to synchronize the update to avoid executing the same computation at the same time. The optimizations are then achieved using scheduling algorithms that evaluates the update jobs and prioritize based on some metric such as the freshness or computation time. These solutions enforce an order of execution, which means some jobs have to wait for others to finish. The only way to optimize the execution of these jobs is to acquire faster CPU's, which is expensive compared to buying more CPU's.

Instead of using advanced scheduling algorithms to achieve a high throughput we suggest a solution that allows concurrent updates, which means we can scale our update execution by adding more processes and thereby be able to execute more jobs at the same time.

To allow concurrent updates we use Timestamp Invalidation as already explained in section~\ref{subsec:timestamp-invalidation}. Timestamp Invalidation ensures that a given computation does not overwrite the value of a cached object if the computation is based on a newer version of underlying data. Figure~\ref{fig:incorrect-update-analysis-timestamp-fix} shows how the timestamp invalidation handles the concurrency challenge described in section~\ref{sec:race-condition-on-write-through-invalidation}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/incorrect-update-analysis-timestamp-fix.pdf}
  \caption{How Invalidation Timestamps fixes the concurrency problem described in figure~\ref{fig:incorrect-updates-analysis}.}
  \label{fig:incorrect-update-analysis-timestamp-fix}
\end{figure*}

Since invalidation timestamps ensures the integrity of the cached objects we are allowed to scale horizontally and thereby execute as many jobs as there are processes available. Beside updating the cached objects concurrently, the algorithm will only schedule new update jobs if a similar job has not already been scheduled. Furthermore when a scheduled update job is processed it will only execute the computation if the cached object is stale.

If a failure happens during the update of a cached object, the algorithm will retry the job by re-scheduling the job. This can be done without additional control mechanisms since timestamp invalidation only require At-Least-Once semantics, which means it allows executing the same job twice without affecting correctness.

% section design-of-the-algorithm end

\section{Implementation of the DUP Algorithm}
\label{sec:implementing-the-data-updata-propagation-algorithm}

Smache implements the data update propagation described above as an extension of the automatic invalidation implementation. After a set of cached objects have been invalidated as illustrated on figure~\ref{fig:background-workers}, Smache schedules the same cached objects to be updated unless an update job for the given cached object already exists.

To be able to work on multiple jobs at the same time, the updates are scheduled asynchronously using the same concurrency model as with concurrent invalidation described in section~\ref{subsec:asynchronous-invalidation} - using background jobs. A cached object is scheduled to be updated by pushing an update job with the name of the cached object as seen on figure~\ref{fig:background-workers}. The background workers will pull the update jobs off the queue and update the cached object unless it is already fresh. Smache updates the cached object by deserializing the name of the cached object using an implementation of \verb$DESERIALIZED-FUN$ as seen in code snippet~\ref{code:function-serialization}, which results in a reference to the cached function and the arguments identifying the cached object. The cached function is then executed with the given arguments and updated in a transaction using timestamp invalidation as described in section~\ref{subsec:update-transactions}. The implementation of this procedure is seen in code snippet~\ref{code:dup-implementation}.

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/dup-background-workers.pdf}
  \caption{How Smache schedules cached objects to be updated using background workers}
  \label{fig:dup-background-workers}
\end{figure*}

\begin{code}{The code for updating a cached object identified by the key (name).}
  \centering
  \input{code/dup_implementation.py}
  \label{code:dup-implementation}
\end{code}

Since we use the same background workers as with automatic invalidation but with different procedures, the DUP algorithm does not require additional components for the architecture seen on figure~\ref{fig:architecture-with-workers}.

% section implementation end

\section{Discussion on Fault-Tolerance}
\label{sec:discussion-on-fault-tolerance}

In the current architecture described on figure~\ref{fig:architecture-with-workers} we assume the web serves and background workers to be redundant, such that if a process fails in some way (e.g. due to network failures), another process is ready to do the task. We do not make any assumptions about the primary storage, so the level of fault-tolerance depends on the setup. The last two components - the queue and the cache database are not redundant.

The queue could fail if there is a network failure such that the web application cannot connect to the queue server. In this case the caching system would not be able to invalidate or schedule updates, which results in invalid freshness evaluation and no cache updates, leading to two major problems. At first we lose the invalidation and update notifications, such that we are not able to recover and restore the application into a consistent state. Secondly, the web application could potentially serve inconsistent cache values to the clients.

We can solve the first problem by having a local proxy queue in the web application that receives the notifications and forwards them to the distributed queue. If a network failure happened, the local queue would still have stored the messages and when the connection is restored, it can resend its messages. The other problem can only be solved by ensuring that we do not serve any cached values, since we cannot be sure if they are consistent. This is an appropriate fallback, but if we recomputed every cached value instead of serving stale values, the application could end up crashing due to high load on the servers.

An advantage of having the queue is that the invalidation and update procedures are executed in isolation. This means if an error or failure happens during the invalidation or update process, then it will not affect the web server behaviour and due to the nature of background jobs, we will be able to retry the procedures easily by rescheduling failed procedures. We can do this without implementing any protocol measures, because the invalidation and updates are designed to be \emph{At-Least-Once} semantics i.e. they must be ensured to be send at least once, but if they are received more than once then it will not affect correctness.

The cache server is also not designed to be redundant since timestamp invalidations require transactions for a given key. We could implement distributed transactions using distributed locking as explained in the Redis documentation~\cite{docs:redis-locking}, but this solution requires all cache servers to be available all the time to ensure consensus across all the cache servers. Although the cache servers cannot be reduntant, we are still able to scale horizontally by using the sharding technique, where the cached objects are distributed across nodes, which is possible because there is no need for interaction between the cached objects.

If the cache server fails in an setup without redundant cache servers, the application would not be able to evaluate freshness or fetch cached values. This should be handled as with failures to the queue, where we could either make the application fail if we want to ensure consistency or just compute the functions normally as before caching was introduced. The appropriate fallback here depends on the use case, which means it could be a declaration made for each cached function whether or not they should be computed on cache failure.

% section discussion-on-fault-tolerance end

\section{Summary}
\label{sec:summary}

In this chapter automatic invalidation was extended to have write-through invalidation such that the cached objects are updated as they are invalidated instead of updating as they are requested. To do this correctly we introduced a Data Update Propagation (DUP) algorithm that uses timestamp invalidation to allow updates to be executed concurrently instead of using a scheduling algorithm. The algorithm was implemented in Python with background workers, which executes the update procedures concurrently with possibility for retries. The solution does not provide a maximum level of fault-tolerance, but the chapter discusses how the solution can be extended with additional measures to achieve a higher level of fault-tolerance.

% section summary end

% section updating_the_cache end
